from collections import deque
from loguru import logger
from multiprocessing.managers import BaseManager
from search import fetch_full_text
import concurrent
import concurrent.futures
import re
from urllib.parse import urlparse, parse_qs
from typing import Dict, List, Tuple, Optional
import numpy as np
from knowledge_graph import build_knowledge_graph



_deepsearch_store = {}

class modelManager(BaseManager): pass
modelManager.register("accessSearchAgents")
modelManager.register("ipcService")
manager = modelManager(address=("localhost", 5010), authkey=b"ipcService")
manager.connect()
search_service = manager.accessSearchAgents()
embedModelService = manager.ipcService()


def cleanQuery(query):
    print("[INFO] Cleaning User Query")
    urls = re.findall(r'(https?://[^\s]+)', query)
    cleaned_query = query
    website_urls = []
    youtube_urls = []

    for url in urls:
        cleaned_query = cleaned_query.replace(url, '').strip()
        url_cleaned = url.rstrip('.,;!?"\'')

        parsed_url = urlparse(url_cleaned)
        if "youtube.com" in parsed_url.netloc or "youtu.be" in parsed_url.netloc:
            youtube_urls.append(url_cleaned)
        elif parsed_url.scheme in ['http', 'https']:
            website_urls.append(url_cleaned)
        cleaned_query = re.sub(r'\s+', ' ', cleaned_query).strip()

    return website_urls, youtube_urls, cleaned_query


def webSearch(query: str):
    urls = search_service.web_search(query)
    return urls

def imageSearch(query: str):
    urls = search_service.image_search(query)
    return urls

def youtubeMetadata(url: str):
    print("[INFO] Getting Youtube Metadata")
    parsed_url = urlparse(url)
    if "youtube.com" not in parsed_url.netloc and "youtu.be" not in parsed_url.netloc:
        print("Not a valid YouTube URL.")
        return None
    try:
        metadata = search_service.get_youtube_metadata(url)
        return metadata
    except Exception as e:
        print(f"Error fetching metadata for {url}: {type(e).__name__} - {e}")
        return None

def preprocess_text(text):
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    text = re.sub(r'[^\w\s.,!?;:]', ' ', text)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    meaningful_sentences = []
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 20 and len(sentence.split()) > 3:
            if not any(word in sentence.lower() for word in ['feedback', 'menu', 'navigation', 'click', 'download']):
                meaningful_sentences.append(sentence)
    
    return meaningful_sentences


def fetch_url_content_parallel(queries, urls, max_workers=10, use_kg: bool = True, request_id: str = None) -> Tuple[str, List[Dict]]:
    """
    Fetch URL content in parallel with optional knowledge graph extraction and request tracking

    Args:
        queries: List of search queries for context
        urls: List of URLs to fetch
        max_workers: Number of parallel workers
        use_kg: Whether to build knowledge graphs
        request_id: Optional request ID for tracking KGs

    Returns:
        Tuple of (aggregated_results_text, kg_data_list)
    """
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_full_text, url, request_id=request_id): url for url in urls}
        results = ""
        kg_data_list = []

        for future in concurrent.futures.as_completed(futures):
            url = futures[future]
            try:
                result = future.result()

                # Handle both old format (string) and new format (tuple)
                if isinstance(result, tuple):
                    text_content, kg_dict = result
                    kg_data_list.append(kg_dict)
                else:
                    text_content = result

                clean_text = str(text_content).encode('unicode_escape').decode('utf-8')
                clean_text = clean_text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')
                clean_text = ''.join(c for c in clean_text if c.isprintable())
                results += f"\nURL: {url}\nText Preview: {clean_text.strip()}"
            except Exception as e:
                logger.error(f"Failed fetching {url}: {e}")
                results += f"\nURL: {url}\n Failed to fetch content of this URL"

        logger.info(f"Fetched all URL information in parallel.")
        information = embedModelService.extract_relevant(results, queries)

        for i in information:
            sentences = []
            for piece in i:
                sentences.extend([s.strip() for s in piece.split('.') if s.strip()])
            results += '. '.join(sentences) + '. '

        return results, kg_data_list


async def rank_results(query: str, results: List[str], ipc_service) -> List[Tuple[str, float]]:
    """
    Rank search results by relevance to query using embeddings.
    
    Args:
        query: The search query
        results: List of result strings to rank
        ipc_service: IPC service with embedding model
        
    Returns:
        List of (result, score) tuples sorted by relevance
    """
    if not results:
        return []
    
    try:
        # Use the ipc_service's rank_results method to avoid IPC serialization issues
        ranked = ipc_service.rank_results(query, results)
        return ranked
    except Exception as e:
        logger.warning(f"Ranking failed: {e}")
        return [(r, 1.0) for r in results]


async def extract_and_rank_sentences(
    url: str,
    content: str,
    query: str,
    ipc_service
) -> List[str]:
    """
    Extract and rank sentences from content by relevance to query.
    
    Args:
        url: Source URL (for logging)
        content: Text content to extract from
        query: Query to rank relevance against
        ipc_service: IPC service with embedding model
        
    Returns:
        List of top-ranked sentences
    """
    try:
        # Use the ipc_service's method to handle sentence extraction server-side
        top_sentences = ipc_service.extract_and_rank_sentences(content, query)
        return top_sentences
    except Exception as e:
        logger.warning(f"Sentence extraction failed for {url}: {e}")
        return []


def build_final_response(
    response_content: str,
    session,
    rag_stats: Dict
) -> str:
    """
    Build final response with content, images, sources, and summary.
    
    Args:
        response_content: Main response content
        session: Session object with metadata
        rag_stats: RAG statistics dictionary
        
    Returns:
        Formatted final response string
    """
    parts = [response_content]
    
    if session.images:
        parts.append("\n\n---\n## Images\n")
        for img_url in session.images[:5]:
            parts.append(f"![](external-image)")
    
    if session.fetched_urls:
        parts.append("\n\n---\n## Sources\n")
        for i, url in enumerate(session.fetched_urls, 1):
            parts.append(f"{i}. [{url}]({url})")
    
    parts.append("\n\n---\n## Summary\n")
    parts.append(f"- Documents: {rag_stats.get('documents_fetched', 0)}")
    parts.append(f"- Entities: {rag_stats.get('entities_extracted', 0)}")
    parts.append(f"- Relationships: {rag_stats.get('relationships_discovered', 0)}")
    
    return "\n".join(parts)



    _deepsearch_store[sessionID] = query

def getDeepSearchQuery(sessionID: str):
    return _deepsearch_store.get(sessionID)

def cleanDeepSearchQuery(sessionID: str):
    if sessionID in _deepsearch_store:
        del _deepsearch_store[sessionID]

def testYoutubeMetadata():
    youtube_url = "https://www.youtube.com/watch?v=FLal-KvTNAQ"
    metadata = youtubeMetadata(youtube_url)
    print("Metadata:", metadata)



def testSearching():
    test_queries = ["Latest news from Nepal", "Political updates in Nepal"]
    test_urls = [
        "https://english.nepalnews.com/",
        "https://apnews.com/article/nepal-gen-z-protests-army-kathmandu-2e4d9e835216b11fa238d7bcf8915cbf",
        "https://www.youtube.com/watch?v=dQw4w9WgXcQ"
    ]
    contents = fetch_url_content_parallel(test_queries, test_urls)
    for idx, content in enumerate(contents):
        print(f"Content snippet {idx+1}:", content[:200])
    
    


if __name__ == "__main__":
    pass